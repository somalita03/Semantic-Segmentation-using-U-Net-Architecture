# -*- coding: utf-8 -*-
"""Semantic Segmentation on the dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RJOyNY1JmSemTyp2JkVjXbBT3cTIFf5S
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install kaggle

! mkdir -p ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d carlolepelaars/camvid

from zipfile import ZipFile
dataset = '/content/camvid.zip'
with ZipFile(dataset,'r') as zip_file:
  zip_file.extractall()
  print("done")

import os
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf

## defining a frame for image and mask storage
  framObjTrain = {'img' : [],
            'mask' : []
            }

  framObjValidation = {'img' : [],
            'mask' : []
            }

  ## defining data Loader function
  def LoadData( frameObj = None, imgPath = None, maskPath = None, shape = 128):
      imgNames = os.listdir(imgPath)
      maskNames = []

      ## generating mask names
      for mem in imgNames:
          maskNames.append(re.sub('\.png', '_L.png', mem))

      imgAddr = imgPath + '/'
      maskAddr = maskPath + '/'

      for i in range (len(imgNames)):
          img = plt.imread(imgAddr + imgNames[i])
          mask = plt.imread(maskAddr + maskNames[i])

          img = cv2.resize(img, (shape, shape))
          mask = cv2.resize(mask, (shape, shape))

          frameObj['img'].append(img)
          frameObj['mask'].append(mask)

      return frameObj

import os
import cv2
import shutil

# Define directories
directories = ['/content/CamVid/train', '/content/CamVid/train_labels', '/content/CamVid/val', '/content/CamVid/val_labels', '/content/CamVid/test', '/content/CamVid/test_labels']
output_dir = 'data/resized_data'  # New directory to save resized images and labels

# Create the output directory
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Resize function
def resize_images(input_dir, output_dir):
    files = os.listdir(input_dir)
    for file in files:
        input_path = os.path.join(input_dir, file)
        output_path = os.path.join(output_dir, file)
        img = cv2.imread(input_path)
        resized_img = cv2.resize(img, (256, 256))  # Resize image to 128x128
        cv2.imwrite(output_path, resized_img)  # Save resized image

# Resize images and labels for all directories
for directory in directories:
    input_dir = directory
    output_subdir = os.path.join(output_dir, os.path.basename(directory))
    if not os.path.exists(output_subdir):
        os.makedirs(output_subdir)
    resize_images(input_dir, output_subdir)

print("Resizing and saving complete.")

import os
import numpy as np
from PIL import Image

# Function to load images from a directory and return as numpy arrays
def load_images_from_directory(directory, target_size=(256, 256), color_mode='rgb'):
    images = []
    for filename in sorted(os.listdir(directory)):
        if filename.endswith('.png') or filename.endswith('.jpg'):
            img_path = os.path.join(directory, filename)
            img = Image.open(img_path)
            if color_mode == 'grayscale':
                img = img.convert('L')  # Convert to grayscale
            else:
                img = img.convert('RGB')  # Convert to RGB
            img = img.resize(target_size)  # Resize to the input size of the model
            img = np.array(img)
            images.append(img)
    return np.array(images)

# Function to load labels from a directory and return as numpy arrays
def load_labels_from_directory(directory, target_size=(256, 256), num_classes=6):
    labels = []
    for filename in sorted(os.listdir(directory)):
        if filename.endswith('.png') or filename.endswith('.jpg'):
            label_path = os.path.join(directory, filename)
            label = Image.open(label_path)
            label = label.convert('L')  # Convert to grayscale label image
            label = label.resize(target_size)  # Resize to the input size of the model
            label = np.array(label)
            label[label >= num_classes] = num_classes - 1  # Ensure label values are within range
            labels.append(label)
    return np.array(labels)

# Directory paths for your datasets
train_images_dir = '/content/path/to/resized_data/train'
train_labels_dir = '/content/path/to/resized_data/train_labels'
val_images_dir = '/content/path/to/resized_data/val'
val_labels_dir = '/content/path/to/resized_data/val_labels'
test_images_dir = '/content/path/to/resized_data/test'
test_labels_dir = '/content/path/to/resized_data/test_labels'

# Load images and labels
train_images = load_images_from_directory(train_images_dir)
train_labels = load_labels_from_directory(train_labels_dir)
val_images = load_images_from_directory(val_images_dir)
val_labels = load_labels_from_directory(val_labels_dir)
test_images = load_images_from_directory(test_images_dir)
test_labels = load_labels_from_directory(test_labels_dir)

# Print shapes
print(f"Train images shape: {train_images.shape}")
print(f"Train labels shape: {train_labels.shape}")
print(f"Validation images shape: {val_images.shape}")
print(f"Validation labels shape: {val_labels.shape}")
print(f"Test images shape: {test_images.shape}")
print(f"Test labels shape: {test_labels.shape}")

framObjTrain = LoadData( framObjTrain, imgPath = '/content/path/to/resized_data/train', maskPath = '/content/path/to/resized_data/train_labels'
         , shape = 256)

framObjValidation  = LoadData( framObjValidation, imgPath = '/content/path/to/resized_data/val', maskPath = '/content/path/to/resized_data/val_labels'
         , shape = 256)

# this block essentially performs 2 convolution
import numpy as np
import tensorflow as tf
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ModelCheckpoint
def Conv2dBlock(inputTensor, numFilters, kernelSize=3, doBatchNorm=True, l2_reg=0.01):
    # Your Conv2dBlock implementation here
     # First convolution
    x = tf.keras.layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),
                               kernel_initializer='he_normal', padding='same',
                               kernel_regularizer=l2(l2_reg))(inputTensor)
    if doBatchNorm:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation('relu')(x)

    # Second convolution
    x = tf.keras.layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),
                               kernel_initializer='he_normal', padding='same',
                               kernel_regularizer=l2(l2_reg))(x)
    if doBatchNorm:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation('relu')(x)

    return x


# Now defining Unet
def GiveMeUnet(inputImage, numFilters=16, dropouts=0.2, doBatchNorm=True, l2_reg=0.01):
    # Your U-Net implementation here
     # Encoder path
    c1 = Conv2dBlock(inputImage, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)
    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)
    p1 = tf.keras.layers.Dropout(dropouts)(p1)

    c2 = Conv2dBlock(p1, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)
    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
    p2 = tf.keras.layers.Dropout(dropouts)(p2)

    c3 = Conv2dBlock(p2, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)
    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
    p3 = tf.keras.layers.Dropout(dropouts)(p3)

    c4 = Conv2dBlock(p3, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)
    p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)
    p4 = tf.keras.layers.Dropout(dropouts)(p4)

    c5 = Conv2dBlock(p4, numFilters * 16, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)

    # Decoder path
    u6 = tf.keras.layers.Conv2DTranspose(numFilters * 8, (3, 3), strides=(2, 2), padding='same')(c5)
    u6 = tf.keras.layers.concatenate([u6, c4])
    u6 = tf.keras.layers.Dropout(dropouts)(u6)
    c6 = Conv2dBlock(u6, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)

    u7 = tf.keras.layers.Conv2DTranspose(numFilters * 4, (3, 3), strides=(2, 2), padding='same')(c6)
    u7 = tf.keras.layers.concatenate([u7, c3])
    u7 = tf.keras.layers.Dropout(dropouts)(u7)
    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)

    u8 = tf.keras.layers.Conv2DTranspose(numFilters * 2, (3, 3), strides=(2, 2), padding='same')(c7)
    u8 = tf.keras.layers.concatenate([u8, c2])
    u8 = tf.keras.layers.Dropout(dropouts)(u8)
    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)

    u9 = tf.keras.layers.Conv2DTranspose(numFilters * 1, (3, 3), strides=(2, 2), padding='same')(c8)
    u9 = tf.keras.layers.concatenate([u9, c1])
    u9 = tf.keras.layers.Dropout(dropouts)(u9)
    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm, l2_reg=l2_reg)

    output = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(c9)
    model = tf.keras.Model(inputs=[inputImage], outputs=[output])
    return model

# Data augmentation
data_augmentation = tf.keras.Sequential([
    # Your data augmentation layers here
    tf.keras.layers.RandomFlip("horizontal_and_vertical"),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
    tf.keras.layers.RandomContrast(0.2),
])

# Example of integrating data augmentation into the input pipeline
inputs = tf.keras.layers.Input((256, 256, 3))
augmented_inputs = data_augmentation(inputs)
myTransformer = GiveMeUnet(augmented_inputs, dropouts=0.3)

myTransformer.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

import numpy as np
import tensorflow as tf

# Assuming myTransformer is your model and framObjTrain is your data
retVal = myTransformer.fit(np.array(framObjTrain['img']), np.array(framObjTrain['mask']), epochs=150, verbose=0)

# Save the trained model
model_save_path = '/content/drive/MyDrive/myTransformer_model.h5'
myTransformer.save(model_save_path)

print(f"Model saved to {model_save_path}")

import tensorflow as tf
import numpy as np
import os
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

def Conv2dBlock(inputTensor, numFilters, kernelSize=3, doBatchNorm=True, dropout=0.1):
    x = tf.keras.layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),
                               kernel_initializer='he_normal', padding='same',
                               kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputTensor)
    if doBatchNorm:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation('relu')(x)
    x = tf.keras.layers.Dropout(dropout)(x)

    x = tf.keras.layers.Conv2D(filters=numFilters, kernel_size=(kernelSize, kernelSize),
                               kernel_initializer='he_normal', padding='same',
                               kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
    if doBatchNorm:
        x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Activation('relu')(x)

    return x

def GiveMeUnet(inputImage, numFilters=16, dropout=0.1, doBatchNorm=True, numClasses=6):
    c1 = Conv2dBlock(inputImage, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)
    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

    c2 = Conv2dBlock(p1, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)
    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)

    c3 = Conv2dBlock(p2, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)
    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)

    c4 = Conv2dBlock(p3, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)
    p4 = tf.keras.layers.MaxPooling2D((2, 2))(c4)

    c5 = Conv2dBlock(p4, numFilters * 16, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)

    u6 = tf.keras.layers.Conv2DTranspose(numFilters * 8, (3, 3), strides=(2, 2), padding='same')(c5)
    u6 = tf.keras.layers.concatenate([u6, c4])
    c6 = Conv2dBlock(u6, numFilters * 8, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)

    u7 = tf.keras.layers.Conv2DTranspose(numFilters * 4, (3, 3), strides=(2, 2), padding='same')(c6)
    u7 = tf.keras.layers.concatenate([u7, c3])
    c7 = Conv2dBlock(u7, numFilters * 4, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)

    u8 = tf.keras.layers.Conv2DTranspose(numFilters * 2, (3, 3), strides=(2, 2), padding='same')(c7)
    u8 = tf.keras.layers.concatenate([u8, c2])
    c8 = Conv2dBlock(u8, numFilters * 2, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)

    u9 = tf.keras.layers.Conv2DTranspose(numFilters * 1, (3, 3), strides=(2, 2), padding='same')(c8)
    u9 = tf.keras.layers.concatenate([u9, c1])
    c9 = Conv2dBlock(u9, numFilters * 1, kernelSize=3, doBatchNorm=doBatchNorm, dropout=dropout)

    output = tf.keras.layers.Conv2D(numClasses, (1, 1), activation='softmax')(c9)
    model = tf.keras.Model(inputs=[inputImage], outputs=[output])
    return model

# Instantiating the model
inputs = tf.keras.layers.Input((256, 256, 3))
myTransformer = GiveMeUnet(inputs, numFilters=16, dropout=0.1, doBatchNorm=True, numClasses=6)
myTransformer.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
callbacks = [
    EarlyStopping(patience=10, verbose=1, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6, verbose=1),
    ModelCheckpoint('best_model.h5', verbose=1, save_best_only=True, save_weights_only=True)
]

def load_images_from_directory(directory, target_size=(256, 256), color_mode='rgb'):
    images = []
    for filename in sorted(os.listdir(directory)):
        if filename.endswith('.png') or filename.endswith('.jpg'):
            img_path = os.path.join(directory, filename)
            img = Image.open(img_path)
            if color_mode == 'grayscale':
                img = img.convert('L')
            else:
                img = img.convert('RGB')
            img = img.resize(target_size)
            img = np.array(img)
            images.append(img)
    return np.array(images)

def normalize_images(images):
    return images.astype('float32') / 255.0

train_images_dir = '/content/path/to/resized_data/train'
train_labels_dir = '/content/path/to/resized_data/train_labels'
val_images_dir = '/content/path/to/resized_data/val'
val_labels_dir = '/content/path/to/resized_data/val_labels'
test_images_dir = '/content/path/to/resized_data/test'
test_labels_dir = '/content/path/to/resized_data/test_labels'

train_images = normalize_images(load_images_from_directory(train_images_dir))
train_labels = load_images_from_directory(train_labels_dir, color_mode='rgb')
val_images = normalize_images(load_images_from_directory(val_images_dir))
val_labels = load_images_from_directory(val_labels_dir, color_mode='rgb')
test_images = normalize_images(load_images_from_directory(test_images_dir))
test_labels = load_images_from_directory(test_labels_dir, color_mode='rgb')

# Ensure label values are within range
num_classes = 32  # Number of classes in your dataset
print(f"Unique label values in train_labels: {np.unique(train_labels)}")
print(f"Unique label values in val_labels: {np.unique(val_labels)}")
print(f"Unique label values in test_labels: {np.unique(test_labels)}")

train_labels[train_labels >= num_classes] = num_classes - 1
val_labels[val_labels >= num_classes] = num_classes - 1
test_labels[test_labels >= num_classes] = num_classes - 1

# Convert labels to categorical
train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)
val_labels = tf.keras.utils.to_categorical(val_labels, num_classes)
test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)

# Data Augmentation
data_gen_args = dict(rotation_range=10,
                     width_shift_range=0.1,
                     height_shift_range=0.1,
                     shear_range=0.1,
                     zoom_range=0.1,
                     horizontal_flip=True,
                     fill_mode='nearest')
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)

# Create data generators
seed = 1
batch_size = 32

def image_mask_generator(image_datagen, mask_datagen, images, masks, batch_size, seed):
    image_generator = image_datagen.flow(images, batch_size=batch_size, seed=seed)
    mask_generator = mask_datagen.flow(masks, batch_size=batch_size, seed=seed)
    while True:
        image_batch = image_generator.next()
        mask_batch = mask_generator.next()
        yield (image_batch, mask_batch)

train_generator = image_mask_generator(image_datagen, mask_datagen, train_images, train_labels, batch_size, seed)
val_generator = image_mask_generator(image_datagen, mask_datagen, val_images, val_labels, batch_size, seed)

# Training the model
model = GiveMeUnet(inputs, numFilters=16, dropout=0.1, doBatchNorm=True, numClasses=num_classes)
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, steps_per_epoch=len(train_images) // batch_size, epochs=150, validation_data=val_generator, validation_steps=len(val_images) // batch_size, callbacks=callbacks)

# Evaluating the model on the test set
test_generator = image_mask_generator(image_datagen, mask_datagen, test_images, test_labels, batch_size, seed)
test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_images) // batch_size)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

import numpy as np
import os
from PIL import Image
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt

# Paths to directories
test_images_dir = '/content/path/to/resized_data/test'
test_labels_dir = '/content/path/to/resized_data/test_labels'

# Load the testing images and labels from directories
def load_images_from_directory(directory):
    images = []
    for file in sorted(os.listdir(directory)):
        if file.endswith('.png'):
            img_path = os.path.join(directory, file)
            img = Image.open(img_path)
            img = np.array(img)
            images.append(img)
    return np.array(images)

test_images = load_images_from_directory(test_images_dir)
test_labels = load_images_from_directory(test_labels_dir)

# Normalize the images
test_images = test_images.astype('float32') / 255.0
test_labels = test_labels.astype('float32') / 255.0

# Load the trained model
model_path = '/content/drive/MyDrive/Colab Notebooks/my_model.h5'
model = load_model(model_path)

# Data augmentation for training (if you have access to training data)
# This is just a placeholder and should be applied during training, not testing
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Callbacks for training (not used during evaluation)
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)
]

# Evaluate the model on the testing dataset in batches
batch_size = 52
num_batches = len(test_images) // batch_size

test_loss_list = []
test_accuracy_list = []

for i in range(num_batches):
    batch_images = test_images[i*batch_size:(i+1)*batch_size]
    batch_labels = test_labels[i*batch_size:(i+1)*batch_size]

    test_loss, test_accuracy = model.evaluate(batch_images, batch_labels, verbose=0)
    test_loss_list.append(test_loss)
    test_accuracy_list.append(test_accuracy)

# Plotting the testing loss and accuracy
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(test_loss_list, label='Test Loss')
plt.xlabel('Batch')
plt.ylabel('Loss')
plt.title('Testing Loss Over Batches')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(test_accuracy_list, label='Test Accuracy')
plt.xlabel('Batch')
plt.ylabel('Accuracy')
plt.title('Testing Accuracy Over Batches')
plt.legend()

plt.tight_layout()
plt.show()

print("Overall Test Loss:", np.mean(test_loss_list))
print("Overall Test Accuracy:", np.mean(test_accuracy_list))

from google.colab import drive
from tensorflow.keras.models import load_model
import numpy as np
import os
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt

# Load the model
model_save_path = '/content/drive/MyDrive/Colab Notebooks/my_model.h5'
model = load_model(model_save_path)

# Paths to test images and masks directories
test_images_dir = '/content/path/to/resized_data/test'
test_masks_dir = '/content/path/to/resized_data/test_labels'

# Function to load images from a directory
def load_images_from_directory(directory, target_size=(256, 256), color_mode='rgb'):
    images = []
    for filename in sorted(os.listdir(directory)):
        if filename.endswith('.png') or filename.endswith('.jpg'):
            img_path = os.path.join(directory, filename)
            img = load_img(img_path, target_size=target_size, color_mode=color_mode)
            img_array = img_to_array(img)
            images.append(img_array)
    return np.array(images)

# Load test images and masks
test_images = load_images_from_directory(test_images_dir)
test_masks = load_images_from_directory(test_masks_dir, color_mode='rgb')

# Normalize test images if needed (optional, depends on your model's training data preprocessing)
test_images = test_images / 255.0

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(test_images, test_masks, verbose=1)

# Print the results
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Predict masks for test images
predicted_masks = model.predict(test_images)

# Ensure predicted masks are in the same range as true masks (0 to 255)
predicted_masks = (predicted_masks * 255).astype('uint8')

# Plot a few test images and their corresponding predicted masks
num_images_to_plot = 5
plt.figure(figsize=(15, 10))
for i in range(num_images_to_plot):
    # Original image
    plt.subplot(3, num_images_to_plot, i + 1)
    plt.imshow(test_images[i].astype('uint8'))
    plt.title("Test Image")
    plt.axis('off')

    # True mask
    plt.subplot(3, num_images_to_plot, i + 1 + num_images_to_plot)
    plt.imshow(test_masks[i].squeeze(), cmap='gray', vmin=0, vmax=255)
    plt.title("True Mask")
    plt.axis('off')

    # Predicted mask
    plt.subplot(3, num_images_to_plot, i + 1 + 2 * num_images_to_plot)
    plt.imshow(predicted_masks[i].squeeze(), cmap='gray', vmin=0, vmax=255)
    plt.title("Predicted Mask")
    plt.axis('off')

plt.tight_layout()
plt.show()

import os
import numpy as np
from PIL import Image  # Use PIL for image handling

# Define color mappings (RGB triplets to object/class names)
color_mappings = {
    (64, 128, 64): 'Animal',
    (192, 0, 128): 'Archway',
    (0, 128, 192): 'Bicyclist',
    (0, 128, 64): 'Bridge',
    (128, 0, 0): 'Building',
    (64, 0, 128): 'Car',
    (64, 0, 192): 'CartLuggagePram',
    (192, 128, 64): 'Child',
    (192, 192, 128): 'Column_Pole',
    (64, 64, 128): 'Fence',
    (128, 0, 192): 'LaneMkgsDriv',
    (192, 0, 64): 'LaneMkgsNonDriv',
    (128, 128, 64): 'Misc_Text',
    (192, 0, 192): 'MotorcycleScooter',
    (128, 64, 64): 'OtherMoving',
    (64, 192, 128): 'ParkingBlock',
    (64, 64, 0): 'Pedestrian',
    (128, 64, 128): 'Road',
    (128, 128, 192): 'RoadShoulder',
    (0, 0, 192): 'Sidewalk',
    (192, 128, 128): 'SignSymbol',
    (128, 128, 128): 'Sky',
    (64, 128, 192): 'SUVPickupTruck',
    (0, 0, 64): 'TrafficCone',
    (0, 64, 64): 'TrafficLight',
    (192, 64, 128): 'Train',
    (128, 128, 0): 'Tree',
    (192, 128, 192): 'Truck_Bus',
    (64, 0, 64): 'Tunnel',
    (192, 192, 0): 'VegetationMisc',
    (0, 0, 0): 'Void',
    (64, 192, 0): 'Wall'
}

# Directories containing annotations/masks
train_dir = '/content/path/to/resized_data/train_labels'
val_dir = '/content/path/to/resized_data/val_labels'
test_dir = '/content/path/to/resized_data/test_labels'

# Function to count occurrences of each object/class in annotations
def count_object_occurrences(annotations_dir, color_mappings):
    object_counts = {obj: 0 for obj in color_mappings.values()}

    # List all annotation files
    annotation_files = sorted(os.listdir(annotations_dir))

    for filename in annotation_files:
        # Load annotation mask
        mask = np.array(Image.open(os.path.join(annotations_dir, filename)))

        # Count occurrences of each object/class
        unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)
        for color in unique_colors:
            color_tuple = tuple(color)
            if color_tuple in color_mappings:
                object_name = color_mappings[color_tuple]
                object_counts[object_name] += np.sum(np.all(mask == color, axis=-1))

    return object_counts

# Count occurrences for each dataset
train_occurrences = count_object_occurrences(train_dir, color_mappings)
val_occurrences = count_object_occurrences(val_dir, color_mappings)
test_occurrences = count_object_occurrences(test_dir, color_mappings)

# Print results
print("Train Set:")
for obj, count in train_occurrences.items():
    print(f"{obj}: {count}")

print("\nValidation Set:")
for obj, count in val_occurrences.items():
    print(f"{obj}: {count}")

print("\nTest Set:")
for obj, count in test_occurrences.items():
    print(f"{obj}: {count}")

import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt  # Import matplotlib for plotting

# Define color mappings (RGB triplets to object/class names)
color_mappings = {
    (64, 128, 64): 'Animal',
    (192, 0, 128): 'Archway',
    (0, 128, 192): 'Bicyclist',
    (0, 128, 64): 'Bridge',
    (128, 0, 0): 'Building',
    (64, 0, 128): 'Car',
    (64, 0, 192): 'CartLuggagePram',
    (192, 128, 64): 'Child',
    (192, 192, 128): 'Column_Pole',
    (64, 64, 128): 'Fence',
    (128, 0, 192): 'LaneMkgsDriv',
    (192, 0, 64): 'LaneMkgsNonDriv',
    (128, 128, 64): 'Misc_Text',
    (192, 0, 192): 'MotorcycleScooter',
    (128, 64, 64): 'OtherMoving',
    (64, 192, 128): 'ParkingBlock',
    (64, 64, 0): 'Pedestrian',
    (128, 64, 128): 'Road',
    (128, 128, 192): 'RoadShoulder',
    (0, 0, 192): 'Sidewalk',
    (192, 128, 128): 'SignSymbol',
    (128, 128, 128): 'Sky',
    (64, 128, 192): 'SUVPickupTruck',
    (0, 0, 64): 'TrafficCone',
    (0, 64, 64): 'TrafficLight',
    (192, 64, 128): 'Train',
    (128, 128, 0): 'Tree',
    (192, 128, 192): 'Truck_Bus',
    (64, 0, 64): 'Tunnel',
    (192, 192, 0): 'VegetationMisc',
    (0, 0, 0): 'Void',
    (64, 192, 0): 'Wall'
}

# Directories containing annotations/masks
train_dir = '/content/path/to/resized_data/train_labels'
val_dir = '/content/path/to/resized_data/val_labels'
test_dir = '/content/path/to/resized_data/test_labels'

# Function to count occurrences of each object/class in annotations
def count_object_occurrences(annotations_dir, color_mappings):
    object_counts = {obj: 0 for obj in color_mappings.values()}

    # List all annotation files
    annotation_files = sorted(os.listdir(annotations_dir))

    for filename in annotation_files:
        # Load annotation mask
        mask = np.array(Image.open(os.path.join(annotations_dir, filename)))

        # Count occurrences of each object/class
        unique_colors = np.unique(mask.reshape(-1, mask.shape[2]), axis=0)
        for color in unique_colors:
            color_tuple = tuple(color)
            if color_tuple in color_mappings:
                object_name = color_mappings[color_tuple]
                object_counts[object_name] += np.sum(np.all(mask == color, axis=-1))

    return object_counts

# Count occurrences for each dataset
train_occurrences = count_object_occurrences(train_dir, color_mappings)
val_occurrences = count_object_occurrences(val_dir, color_mappings)
test_occurrences = count_object_occurrences(test_dir, color_mappings)

# Print results
print("Train Set:")
for obj, count in train_occurrences.items():
    print(f"{obj}: {count}")

print("\nValidation Set:")
for obj, count in val_occurrences.items():
    print(f"{obj}: {count}")

print("\nTest Set:")
for obj, count in test_occurrences.items():
    print(f"{obj}: {count}")

# Plotting histograms
def plot_histogram(data, title):
    objects = list(data.keys())
    counts = list(data.values())

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(objects, counts, color='skyblue')
    ax.set_xlabel('Object/Class')
    ax.set_ylabel('Count')
    ax.set_title(title)
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

plot_histogram(train_occurrences, 'Train Set Object Occurrences')
plot_histogram(val_occurrences, 'Validation Set Object Occurrences')
plot_histogram(test_occurrences, 'Test Set Object Occurrences')

# Counts of pixels for each class in each dataset split
train_counts = {
    'Animal': 1024,
    'Archway': 10294,
    'Bicyclist': 107609,
    'Bridge': 10003,
    'Building': 5420099,
    'Car': 735036,
    'CartLuggagePram': 5376,
    'Child': 6478,
    'Column_Pole': 183203,
    'Fence': 390010,
    'LaneMkgsDriv': 358720,
    'LaneMkgsNonDriv': 1987,
    'Misc_Text': 148195,
    'MotorcycleScooter': 1226,
    'OtherMoving': 104336,
    'ParkingBlock': 80991,
    'Pedestrian': 132342,
    'Road': 6772458,
    'RoadShoulder': 55837,
    'Sidewalk': 1605098,
    'SignSymbol': 28210,
    'Sky': 3515234,
    'SUVPickupTruck': 124469,
    'TrafficCone': 804,
    'TrafficLight': 80566,
    'Train': 1,
    'Tree': 2354496,
    'Truck_Bus': 43406,
    'Tunnel': 0,
    'VegetationMisc': 189040,
    'Void': 531599,
    'Wall': 283002
}

val_counts = {
    'Animal': 186,
    'Archway': 14070,
    'Bicyclist': 29960,
    'Bridge': 1652,
    'Building': 1443092,
    'Car': 231363,
    'CartLuggagePram': 2189,
    'Child': 1120,
    'Column_Pole': 47894,
    'Fence': 79614,
    'LaneMkgsDriv': 86881,
    'LaneMkgsNonDriv': 2574,
    'Misc_Text': 36348,
    'MotorcycleScooter': 371,
    'OtherMoving': 21686,
    'ParkingBlock': 14785,
    'Pedestrian': 35152,
    'Road': 1850279,
    'RoadShoulder': 25134,
    'Sidewalk': 380177,
    'SignSymbol': 6445,
    'Sky': 990616,
    'SUVPickupTruck': 47432,
    'TrafficCone': 319,
    'TrafficLight': 25421,
    'Train': 0,
    'Tree': 644174,
    'Truck_Bus': 27213,
    'Tunnel': 0,
    'VegetationMisc': 59264,
    'Void': 129057,
    'Wall': 74423
}

test_counts = {
    'Animal': 768,
    'Archway': 3008,
    'Bicyclist': 89587,
    'Bridge': 8371,
    'Building': 3412886,
    'Car': 559967,
    'CartLuggagePram': 4253,
    'Child': 3396,
    'Column_Pole': 97922,
    'Fence': 162465,
    'LaneMkgsDriv': 167978,
    'LaneMkgsNonDriv': 285,
    'Misc_Text': 65185,
    'MotorcycleScooter': 2751,
    'OtherMoving': 41850,
    'ParkingBlock': 49433,
    'Pedestrian': 91978,
    'Road': 3671888,
    'RoadShoulder': 30818,
    'Sidewalk': 846607,
    'SignSymbol': 14455,
    'Sky': 2518082,
    'SUVPickupTruck': 141352,
    'TrafficCone': 129,
    'TrafficLight': 50748,
    'Train': 0,
    'Tree': 1619122,
    'Truck_Bus': 171156,
    'Tunnel': 0,
    'VegetationMisc': 86165,
    'Void': 522257,
    'Wall': 233664
}

# Function to calculate class-wise accuracies
def calculate_class_accuracies(counts):
    accuracies = {}
    for obj, total_pixels in counts.items():
        accuracies[obj] = total_pixels / sum(counts.values()) * 100
    return accuracies

# Calculate class-wise accuracies for each dataset split
train_accuracies = calculate_class_accuracies(train_counts)
val_accuracies = calculate_class_accuracies(val_counts)
test_accuracies = calculate_class_accuracies(test_counts)


print("Accuracies:")
for obj, acc in test_accuracies.items():
    print(f"{obj}: {acc:.2f}%")

import matplotlib.pyplot as plt

# Function to calculate class-wise accuracies
def calculate_class_accuracies(counts):
    accuracies = {}
    for obj, total_pixels in counts.items():
        accuracies[obj] = total_pixels / sum(counts.values()) * 100
    return accuracies

# Calculate class-wise accuracies for each dataset split
train_accuracies = calculate_class_accuracies(train_counts)
val_accuracies = calculate_class_accuracies(val_counts)
test_accuracies = calculate_class_accuracies(test_counts)

# Function to plot histogram for accuracies
def plot_accuracies(accuracies, title):
    objects = list(accuracies.keys())
    values = list(accuracies.values())

    plt.figure(figsize=(12, 8))
    plt.barh(objects, values, color='skyblue')
    plt.xlabel('Accuracy (%)')
    plt.title(title)
    plt.grid(axis='x')
    plt.tight_layout()
    plt.show()

# Plot histograms for each dataset split
plot_accuracies(train_accuracies, 'Train Set Class-Wise Accuracies')
plot_accuracies(val_accuracies, 'Validation Set Class-Wise Accuracies')
plot_accuracies(test_accuracies, 'Test Set Class-Wise Accuracies')

from google.colab import drive
drive.mount('/content/drive')